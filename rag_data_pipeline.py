import os
import time
from thordata_sdk import ThordataClient
from bs4 import BeautifulSoup
from dotenv import load_dotenv # pip install python-dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

def clean_html_to_markdown(html_content):
    """
    ç®€å•çš„ ETL å‡½æ•°ï¼šå°†æ‚ä¹±çš„ HTML æ¸…æ´—ä¸º AI æ˜“è¯»çš„ Markdown æ ¼å¼
    """
    soup = BeautifulSoup(html_content, "html.parser")
    
    # 1. ç§»é™¤æ— å…³æ ‡ç­¾ (å¹¿å‘Šã€å¯¼èˆªã€è„šæœ¬)
    for tag in soup(["script", "style", "nav", "footer", "iframe", "noscript"]):
        tag.decompose()
        
    # 2. æå–æ ‡é¢˜å’Œæ­£æ–‡
    markdown_lines = []
    
    # æå– H1-H6 æ ‡é¢˜
    for heading in soup.find_all(["h1", "h2", "h3"]):
        prefix = "#" * int(heading.name[1])
        markdown_lines.append(f"\n{prefix} {heading.get_text().strip()}\n")
        
    # æå–æ®µè½
    for p in soup.find_all("p"):
        text = p.get_text().strip()
        if len(text) > 20: # è¿‡æ»¤å¤ªçŸ­çš„åºŸè¯
            markdown_lines.append(text)
            
    return "\n".join(markdown_lines)

def main():
    # 1. åˆå§‹åŒ–å®¢æˆ·ç«¯
    scraper_token = os.getenv("THORDATA_SCRAPER_TOKEN")
    public_token = os.getenv("THORDATA_PUBLIC_TOKEN")
    public_key = os.getenv("THORDATA_PUBLIC_KEY")
    
    if not scraper_token:
        print("âŒ Error: .env file not found or missing tokens.")
        return

    client = ThordataClient(scraper_token, public_token, public_key)
    
    # 2. è®¾ç½®ç›®æ ‡ (ä»¥ OpenAI åšå®¢ä¸ºä¾‹ï¼Œå› ä¸ºå¾ˆå¤š AI å…¬å¸æƒ³æŠ“è¿™ä¸ª)
    target_url = "https://openai.com/research/" 
    print(f"ğŸš€ Starting RAG Pipeline for: {target_url}")
    
    try:
        # 3. ä½¿ç”¨ Universal API æŠ“å– (è‡ªåŠ¨æ¸²æŸ“ JSï¼Œç»•è¿‡åçˆ¬)
        print("   Requesting Universal Scraper...")
        html = client.universal_scrape(
            url=target_url,
            js_render=True, # å¿…é¡»å¼€å¯ï¼Œç°ä»£ç½‘ç«™å¤§å¤šæ˜¯åŠ¨æ€çš„
            output_format="HTML"
        )
        print(f"âœ… Scrape Success! Length: {len(html)} chars")
        
        # 4. æ•°æ®æ¸…æ´— (ETL)
        print("   Processing data...")
        markdown_content = clean_html_to_markdown(html)
        
        # 5. ä¿å­˜ç»“æœ
        output_file = "knowledge_base_sample.md"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"Source: {target_url}\n\n")
            f.write(markdown_content)
            
        print(f"ğŸ‰ Pipeline Completed! Data saved to '{output_file}'")
        print("   (This file is ready for Vector Database embedding)")
        
    except Exception as e:
        print(f"âŒ Pipeline Failed: {e}")

if __name__ == "__main__":
    main()