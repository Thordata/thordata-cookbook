{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ec5bae-47b3-408f-9923-32ad6d583344",
   "metadata": {},
   "source": [
    "# RAG Data Pipeline: OpenAI Research Page\n",
    "\n",
    "This notebook demonstrates how to use **Thordata's Universal Scraping API**\n",
    "to fetch a dynamic web page, clean the HTML into LLM‑friendly text, and save\n",
    "it as a Markdown file ready for vector database ingestion.\n",
    "\n",
    "We support two modes:\n",
    "\n",
    "- **Live mode** (`USE_LIVE_THORDATA = True`): calls Thordata APIs and consumes credits.\n",
    "- **Offline mode** (`USE_LIVE_THORDATA = False`): loads data from local cache files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc319dd-9e40-4f93-96df-0f0e4624aa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from thordata import ThordataClient\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "load_dotenv()  # Load THORDATA_* tokens from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71840422-26a1-4098-81eb-ec82f4889b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: D:\\Thordata_Work\\thordata-cookbook\\notebooks\\rag\n",
      "ROOT_DIR: D:\\Thordata_Work\\thordata-cookbook\n",
      "USE_LIVE_THORDATA: False\n",
      "RAW_HTML_PATH: D:\\Thordata_Work\\thordata-cookbook\\data\\openai_research_raw.html\n",
      "CLEAN_MD_FALLBACK_PATH: D:\\Thordata_Work\\thordata-cookbook\\knowledge_base_sample.md\n"
     ]
    }
   ],
   "source": [
    "# Resolve project root from this notebook location:\n",
    "# notebooks/rag -> parent = notebooks -> parent = repo root\n",
    "ROOT_DIR = Path.cwd().parents[1]\n",
    "\n",
    "# Toggle between live API calls and local cached data.\n",
    "# Set to True only when you want to consume Thordata credits.\n",
    "USE_LIVE_THORDATA = False\n",
    "\n",
    "# Cache directory for raw HTML (under repo root)\n",
    "CACHE_DIR = ROOT_DIR / \"data\"\n",
    "RAW_HTML_PATH = CACHE_DIR / \"openai_research_raw.html\"\n",
    "\n",
    "# Fallback: cleaned markdown generated by previous scripts or runs.\n",
    "# This file is expected to be located in the project root.\n",
    "CLEAN_MD_FALLBACK_PATH = ROOT_DIR / \"knowledge_base_sample.md\"\n",
    "\n",
    "target_url = \"https://openai.com/research/\"\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"ROOT_DIR:\", ROOT_DIR)\n",
    "print(\"USE_LIVE_THORDATA:\", USE_LIVE_THORDATA)\n",
    "print(\"RAW_HTML_PATH:\", RAW_HTML_PATH)\n",
    "print(\"CLEAN_MD_FALLBACK_PATH:\", CLEAN_MD_FALLBACK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c414753-be59-419a-b936-16925fdca661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_to_markdown(html_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert messy HTML into Markdown-style text suitable for RAG / LLMs.\n",
    "\n",
    "    - Remove scripts, styles, navigation, footers, iframes, etc.\n",
    "    - Collect headings (h1–h3) and reasonably long paragraphs.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Remove irrelevant tags\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"iframe\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    markdown_lines: List[str] = []\n",
    "\n",
    "    # Headings (H1–H3)\n",
    "    for heading in soup.find_all([\"h1\", \"h2\", \"h3\"]):\n",
    "        level = int(heading.name[1])\n",
    "        prefix = \"#\" * level\n",
    "        markdown_lines.append(f\"\\n{prefix} {heading.get_text(strip=True)}\\n\")\n",
    "\n",
    "    # Paragraphs\n",
    "    for p in soup.find_all(\"p\"):\n",
    "        text = p.get_text(strip=True)\n",
    "        if len(text) > 20:\n",
    "            markdown_lines.append(text)\n",
    "\n",
    "    return \"\\n\".join(markdown_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629e13d9-4b09-4a23-9da8-d7431fa42f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<thordata.client.ThordataClient at 0x1af083512b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCRAPER_TOKEN = os.getenv(\"THORDATA_SCRAPER_TOKEN\")\n",
    "PUBLIC_TOKEN = os.getenv(\"THORDATA_PUBLIC_TOKEN\")\n",
    "PUBLIC_KEY = os.getenv(\"THORDATA_PUBLIC_KEY\")\n",
    "\n",
    "if not SCRAPER_TOKEN:\n",
    "    raise ValueError(\"Please set THORDATA_* variables in your .env file.\")\n",
    "\n",
    "client = ThordataClient(\n",
    "    scraper_token=SCRAPER_TOKEN,\n",
    "    public_token=PUBLIC_TOKEN,\n",
    "    public_key=PUBLIC_KEY,\n",
    ")\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018c7095-8fd6-4757-9a91-34fe62f35da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned markdown directly from D:\\Thordata_Work\\thordata-cookbook\\knowledge_base_sample.md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2795"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if USE_LIVE_THORDATA:\n",
    "    # Live mode: call Thordata Universal API and cache raw HTML\n",
    "    print(f\"Fetching page from Thordata: {target_url}\")\n",
    "    html = client.universal_scrape(\n",
    "        url=target_url,\n",
    "        js_render=True,\n",
    "        output_format=\"HTML\",\n",
    "    )\n",
    "\n",
    "    os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "    with open(RAW_HTML_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    print(f\"Cached raw HTML to {RAW_HTML_PATH}\")\n",
    "\n",
    "    # Clean HTML into markdown\n",
    "    markdown_content = clean_html_to_markdown(html)\n",
    "else:\n",
    "    # Offline mode:\n",
    "    # 1) Prefer an existing cleaned markdown file if present\n",
    "    if os.path.exists(CLEAN_MD_FALLBACK_PATH):\n",
    "        print(f\"Loading cleaned markdown directly from {CLEAN_MD_FALLBACK_PATH}\")\n",
    "        with open(CLEAN_MD_FALLBACK_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            markdown_content = f.read()\n",
    "    # 2) Otherwise, try to load cached raw HTML and clean it\n",
    "    elif os.path.exists(RAW_HTML_PATH):\n",
    "        print(f\"Loading cached HTML from {RAW_HTML_PATH}\")\n",
    "        with open(RAW_HTML_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            html = f.read()\n",
    "        markdown_content = clean_html_to_markdown(html)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No cached HTML ({RAW_HTML_PATH}) or fallback markdown \"\n",
    "            f\"({CLEAN_MD_FALLBACK_PATH}) found. \"\n",
    "            \"Run in live mode once to create them.\"\n",
    "        )\n",
    "\n",
    "len(markdown_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896fd7eb-f4a4-4430-a962-537350a1cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: https://openai.com/research/\n",
      "\n",
      "Source: https://openai.com/research/\n",
      "\n",
      "\n",
      "# Pioneering research on the path to AGI\n",
      "\n",
      "\n",
      "### We use Deep Learning to leverage large amounts of data and advanced reasoning to train AI systems for task completion.\n",
      "\n",
      "\n",
      "### GPT\n",
      "\n",
      "\n",
      "### o series\n",
      "\n",
      "\n",
      "### Visual\n",
      "\n",
      "\n",
      "### Audio\n",
      "\n",
      "\n",
      "### Text\n",
      "\n",
      "\n",
      "### Featured roles\n",
      "\n",
      "\n",
      "### Research Engineer\n",
      "\n",
      "\n",
      "### Research Engineer, Codex\n",
      "\n",
      "\n",
      "### Research Engineer, Frontier Evals & Environments\n",
      "\n",
      "\n",
      "### Research Scientist\n",
      "\n",
      "\n",
      "## Explore all research\n",
      "\n",
      "We believe our research will eventually lead to artificial general intelligence, a system that can solve human-level problems. Our mission is to ensure that AGI benefits all of humanity.\n",
      "OpenAI’s GPT series models are fast, versatile, and cost-efficient AI systems designed to understand context, generate content, and reason across text, images, and more.\n",
      "A smarter, more conversational ChatGPT\n",
      "ReleaseNov 12, 20258 min read\n",
      "Our smartest, fastest, and most useful model yet, with thinking built in\n",
      "ReleaseAug 7, \n"
     ]
    }
   ],
   "source": [
    "print(markdown_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "484665fe-c752-4fbd-938e-adc067675285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned markdown to D:\\Thordata_Work\\thordata-cookbook\\data\\knowledge_base_20251128_100915.md\n"
     ]
    }
   ],
   "source": [
    "# 使用时间戳生成不重复的名称\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = CACHE_DIR / f\"knowledge_base_{timestamp}.md\"  # 存在 data/ 里\n",
    "\n",
    "with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Source: {target_url}\\n\\n\")\n",
    "    f.write(markdown_content)\n",
    "\n",
    "print(f\"Saved cleaned markdown to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f19fd-caa8-445c-bf83-e61ddd108638",
   "metadata": {},
   "source": [
    "The file `knowledge_base_sample.md` is now ready to be embedded into a vector\n",
    "database (e.g. Pinecone, Weaviate, Chroma) and used in a RAG pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
