{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22be0e60-cf5c-4978-9be2-3c64c9a97b6a",
   "metadata": {},
   "source": [
    "# Web Q&A Agent with Thordata (SERP + Universal + LLM)\n",
    "\n",
    "This notebook shows how to build a simple **Web Question‑Answering Agent**:\n",
    "\n",
    "1. Take a natural language question.\n",
    "2. Use **Thordata SERP API** to search the web (Google by default).\n",
    "3. For the top results, use **Thordata Universal Scraping API** to fetch and clean\n",
    "   the page content.\n",
    "4. Call an LLM (e.g. OpenAI) to generate an answer based on the retrieved pages,\n",
    "   including citations/links.\n",
    "\n",
    "We also support:\n",
    "\n",
    "- **Live mode** (`USE_LIVE_THORDATA = True`) to call real APIs.\n",
    "- **Offline mode** (`USE_LIVE_THORDATA = False`) to load cached documents\n",
    "  from `data/web_qa_sample.json` without consuming credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c83a73-ce41-4e53-a068-b76bfc36410d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: D:\\Thordata_Work\\thordata-cookbook\\notebooks\\ai\n",
      "ROOT_DIR: D:\\Thordata_Work\\thordata-cookbook\n",
      "ENV_PATH exists? -> True\n",
      "DOCS_CACHE_PATH: D:\\Thordata_Work\\thordata-cookbook\\data\\web_qa_sample.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from thordata import Engine, ThordataClient\n",
    "\n",
    "# Optional: try to import OpenAI for LLM\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    OpenAI = None  # We will handle this gracefully later\n",
    "\n",
    "# -----------------------------\n",
    "# Resolve project root\n",
    "# -----------------------------\n",
    "# 情况 1：在 .py 脚本里，有 __file__，可以直接从脚本位置往上算\n",
    "if \"__file__\" in globals():\n",
    "    ROOT_DIR = Path(__file__).resolve().parents[2]\n",
    "else:\n",
    "    # 情况 2：在 Notebook 里，当前工作目录是 notebooks/ai\n",
    "    # notebooks/ai  -> parent(ai) -> parent(notebooks) -> parent(仓库根)\n",
    "    ROOT_DIR = Path.cwd().parents[1]\n",
    "\n",
    "# Load .env from the project root\n",
    "ENV_PATH = ROOT_DIR / \".env\"\n",
    "load_dotenv(ENV_PATH, override=True)\n",
    "\n",
    "# Toggle between live API calls and local cached data\n",
    "USE_LIVE_THORDATA = True\n",
    "\n",
    "# Cache path for web QA documents (统一放在仓库根目录的 data/)\n",
    "CACHE_DIR = ROOT_DIR / \"data\"\n",
    "DOCS_CACHE_PATH = CACHE_DIR / \"web_qa_sample.json\"\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"ROOT_DIR:\", ROOT_DIR)\n",
    "print(\"ENV_PATH exists? ->\", ENV_PATH.is_file())\n",
    "print(\"DOCS_CACHE_PATH:\", DOCS_CACHE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd83aa31-319b-4cce-8be2-0ae79e79668a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<thordata.client.ThordataClient at 0x14a6ee0fb60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCRAPER_TOKEN = os.getenv(\"THORDATA_SCRAPER_TOKEN\")\n",
    "PUBLIC_TOKEN = os.getenv(\"THORDATA_PUBLIC_TOKEN\")\n",
    "PUBLIC_KEY = os.getenv(\"THORDATA_PUBLIC_KEY\")\n",
    "\n",
    "if not SCRAPER_TOKEN:\n",
    "    raise RuntimeError(\n",
    "        \"THORDATA_SCRAPER_TOKEN is missing. \"\n",
    "        \"Please configure your .env file at the project root.\"\n",
    "    )\n",
    "\n",
    "td_client = ThordataClient(\n",
    "    scraper_token=SCRAPER_TOKEN,\n",
    "    public_token=PUBLIC_TOKEN,\n",
    "    public_key=PUBLIC_KEY,\n",
    ")\n",
    "\n",
    "td_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d566ca6-2553-441e-a6f8-e0c97b13d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web_serp(\n",
    "    query: str,\n",
    "    num_results: int = 3,\n",
    "    engine: Engine = Engine.GOOGLE,\n",
    "    location: str | None = None,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Use Thordata SERP API to search the web and return a list of basic results.\n",
    "    \"\"\"\n",
    "    extra_params: dict[str, Any] = {}\n",
    "    if location:\n",
    "        extra_params[\"location\"] = location\n",
    "\n",
    "    print(f\"Searching {engine.value} for: {query!r}\")\n",
    "    results = td_client.serp_search(\n",
    "        query=query,\n",
    "        engine=engine,\n",
    "        num=num_results,\n",
    "        **extra_params,\n",
    "    )\n",
    "\n",
    "    organic = results.get(\"organic\") or []\n",
    "    cleaned: list[dict[str, Any]] = []\n",
    "    for item in organic:\n",
    "        cleaned.append(\n",
    "            {\n",
    "                \"title\": item.get(\"title\"),\n",
    "                \"link\": item.get(\"link\"),\n",
    "                \"snippet\": item.get(\"snippet\"),\n",
    "            }\n",
    "        )\n",
    "    print(f\"Got {len(cleaned)} organic results.\")\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5be855-2791-4688-afa5-d072e0796c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_to_text(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert raw HTML into a cleaned plain-text representation.\n",
    "\n",
    "    - Removes scripts, styles, navigation, footers, SVGs, iframes.\n",
    "    - Collapses whitespace and drops empty lines.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Remove noisy elements\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"svg\", \"iframe\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    clean_text = \"\\n\".join(lines)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "638800ad-7bd8-44b5-ac3d-77e938508e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_docs_from_web(\n",
    "    query: str,\n",
    "    num_results: int = 3,\n",
    "    engine: Engine = Engine.GOOGLE,\n",
    "    location: str | None = None,\n",
    "    js_render: bool = True,\n",
    "    per_doc_max_chars: int = 4000,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    High-level function: search the web and fetch cleaned text for top results.\n",
    "\n",
    "    Returns:\n",
    "        List of documents: [{ \"url\", \"title\", \"snippet\", \"content\" }, ...]\n",
    "    \"\"\"\n",
    "    serp_results = search_web_serp(query, num_results=num_results, engine=engine, location=location)\n",
    "\n",
    "    docs: list[dict[str, Any]] = []\n",
    "    for idx, item in enumerate(serp_results, start=1):\n",
    "        url = item.get(\"link\")\n",
    "        if not url:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[{idx}/{len(serp_results)}] Fetching via Universal API: {url}\")\n",
    "        html = td_client.universal_scrape(\n",
    "            url=url,\n",
    "            js_render=js_render,\n",
    "            output_format=\"HTML\",\n",
    "        )\n",
    "\n",
    "        if not html or len(html) < 200:\n",
    "            print(\"  Skipping: content too short or empty.\")\n",
    "            continue\n",
    "\n",
    "        text = clean_html_to_text(html)\n",
    "        if per_doc_max_chars and len(text) > per_doc_max_chars:\n",
    "            text = text[:per_doc_max_chars]\n",
    "\n",
    "        docs.append(\n",
    "            {\n",
    "                \"url\": url,\n",
    "                \"title\": item.get(\"title\"),\n",
    "                \"snippet\": item.get(\"snippet\"),\n",
    "                \"content\": text,\n",
    "            }\n",
    "        )\n",
    "        print(f\"  Collected {len(text)} characters of cleaned text.\")\n",
    "\n",
    "    print(f\"\\nTotal documents collected: {len(docs)}\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06941bcd-620b-4abc-a6fa-5ebf715ac6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching google for: 'What is Thordata used for?'\n",
      "Got 3 organic results.\n",
      "\n",
      "[1/3] Fetching via Universal API: https://www.thordata.com/\n",
      "  Collected 4000 characters of cleaned text.\n",
      "\n",
      "[2/3] Fetching via Universal API: https://www.thordata.com/blog/proxies/thordata-review\n",
      "  Collected 4000 characters of cleaned text.\n",
      "\n",
      "[3/3] Fetching via Universal API: https://www.youtube.com/watch?v=9k10yMGKQAE\n",
      "  Collected 4000 characters of cleaned text.\n",
      "\n",
      "Total documents collected: 3\n",
      "\n",
      "Cached docs to D:\\Thordata_Work\\thordata-cookbook\\data\\web_qa_sample.json\n",
      "Loaded 3 documents.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thordata - High-Quality Proxy Service for Web ...</td>\n",
       "      <td>https://www.thordata.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Best Proxies for AI and Streaming</td>\n",
       "      <td>https://www.thordata.com/blog/proxies/thordata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is This The BEST Web Scraping/Proxy Platform f...</td>\n",
       "      <td>https://www.youtube.com/watch?v=9k10yMGKQAE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Thordata - High-Quality Proxy Service for Web ...   \n",
       "1              The Best Proxies for AI and Streaming   \n",
       "2  Is This The BEST Web Scraping/Proxy Platform f...   \n",
       "\n",
       "                                                 url  \n",
       "0                          https://www.thordata.com/  \n",
       "1  https://www.thordata.com/blog/proxies/thordata...  \n",
       "2        https://www.youtube.com/watch?v=9k10yMGKQAE  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_docs_for_question(\n",
    "    question: str,\n",
    "    num_results: int = 3,\n",
    "    engine: Engine = Engine.GOOGLE,\n",
    "    location: str | None = None,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Orchestrator: in live mode, fetch docs from web and cache them.\n",
    "    In offline mode, load docs from the local cache.\n",
    "    \"\"\"\n",
    "    if USE_LIVE_THORDATA:\n",
    "        docs = fetch_docs_from_web(\n",
    "            query=question,\n",
    "            num_results=num_results,\n",
    "            engine=engine,\n",
    "            location=location,\n",
    "            js_render=True,\n",
    "            per_doc_max_chars=4000,\n",
    "        )\n",
    "        # Cache to local JSON\n",
    "        CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        with DOCS_CACHE_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nCached docs to {DOCS_CACHE_PATH}\")\n",
    "    else:\n",
    "        print(f\"Loading docs from cache: {DOCS_CACHE_PATH}\")\n",
    "        if not DOCS_CACHE_PATH.is_file():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Cached docs not found at {DOCS_CACHE_PATH}. \"\n",
    "                \"Set USE_LIVE_THORDATA = True and run once to create them.\"\n",
    "            )\n",
    "        with DOCS_CACHE_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            docs = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "# Example: run once to see the structure (will use cache or live depending on flag)\n",
    "sample_question = \"What is Thordata used for?\"\n",
    "docs = get_docs_for_question(sample_question, num_results=3)\n",
    "pd.DataFrame(\n",
    "    [{\"title\": d[\"title\"], \"url\": d[\"url\"]} for d in docs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43548028-2207-4ef8-8f95-63a2d4956292",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def summarize_with_llm(\n",
    "    question: str,\n",
    "    docs: list[dict[str, Any]],\n",
    "    model: str = \"gpt-4o-mini\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Ask an LLM to answer the question based on the provided documents.\n",
    "\n",
    "    If OpenAI is not installed or API key is missing, a helpful message is returned.\n",
    "    \"\"\"\n",
    "    if OpenAI is None:\n",
    "        return (\n",
    "            \"LLM backend is not configured. Please install the 'openai' package:\\n\"\n",
    "            \"  pip install openai\\n\"\n",
    "            \"and set OPENAI_API_KEY in your .env file.\"\n",
    "        )\n",
    "\n",
    "    if not OPENAI_API_KEY:\n",
    "        return (\n",
    "            \"OPENAI_API_KEY is missing. Please set it in your .env file to enable LLM calls.\"\n",
    "        )\n",
    "\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "    # Build context from docs\n",
    "    context_parts = []\n",
    "    for idx, doc in enumerate(docs, start=1):\n",
    "        context_parts.append(\n",
    "            f\"[Source {idx}] {doc.get('title')}\\n\"\n",
    "            f\"URL: {doc.get('url')}\\n\"\n",
    "            f\"Snippet: {doc.get('snippet')}\\n\"\n",
    "            f\"Content:\\n{doc.get('content')}\\n\"\n",
    "        )\n",
    "    context_text = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a helpful web research assistant. \"\n",
    "        \"Use ONLY the provided sources to answer the user's question. \"\n",
    "        \"Include citations like [1], [2] that refer to the sources listed at the end.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"Question:\\n{question}\\n\\n\"\n",
    "        f\"Sources:\\n{context_text}\\n\\n\"\n",
    "        \"Please provide a concise answer (in English or the question's language), \"\n",
    "        \"with citations [1], [2], etc. Then list the sources with their URLs.\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c850cfd-e659-483a-a8e2-d8a9e1786b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching google for: 'What are the main use cases of Thordata for AI data pipelines?'\n",
      "Got 3 organic results.\n",
      "\n",
      "[1/3] Fetching via Universal API: https://www.thordata.com/blog/proxies/thordata-review\n",
      "  Collected 4000 characters of cleaned text.\n",
      "\n",
      "[2/3] Fetching via Universal API: https://skywork.ai/skypage/en/Thordata:-Unlocking-AI's-Potential---A-Comprehensive-2025-Guide-for-AI-Professionals/1972882733226061824\n",
      "  Skipping: content too short or empty.\n",
      "\n",
      "[3/3] Fetching via Universal API: https://techbullion.com/is-thordata-the-next-big-name-in-enterprise-data-infrastructure-a-comprehensive-look/\n",
      "  Collected 4000 characters of cleaned text.\n",
      "\n",
      "Total documents collected: 2\n",
      "\n",
      "Cached docs to D:\\Thordata_Work\\thordata-cookbook\\data\\web_qa_sample.json\n",
      "Loaded 2 documents.\n",
      "\n",
      "Collected 2 docs. Asking LLM...\n",
      "\n",
      "=== LLM Answer ===\n",
      "\n",
      "LLM backend is not configured. Please install the 'openai' package:\n",
      "  pip install openai\n",
      "and set OPENAI_API_KEY in your .env file.\n"
     ]
    }
   ],
   "source": [
    "# You can change this question to anything you like\n",
    "question = \"What are the main use cases of Thordata for AI data pipelines?\"\n",
    "\n",
    "docs = get_docs_for_question(\n",
    "    question,\n",
    "    num_results=3,\n",
    "    engine=Engine.GOOGLE,\n",
    "    location=None,  # e.g. \"United States\"\n",
    ")\n",
    "\n",
    "print(f\"\\nCollected {len(docs)} docs. Asking LLM...\")\n",
    "\n",
    "answer = summarize_with_llm(question, docs)\n",
    "print(\"\\n=== LLM Answer ===\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c719da4b-bbd0-4efb-857a-b2d87181de9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
